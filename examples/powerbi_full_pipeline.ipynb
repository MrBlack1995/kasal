{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9771f72-fb59-4879-962a-eb40f27dbecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPower BI Full Pipeline - Metadata Extraction, DAX Generation, and Execution\\n\\nThis notebook provides an end-to-end Power BI integration in one place:\\n1. Extract metadata from Power BI semantic model\\n2. Generate DAX query from natural language question using LLM\\n3. Execute the generated DAX query\\n4. Return all results\\n\\nRequired Parameters (via job_params):\\n- workspace_id: Power BI workspace ID\\n- semantic_model_id: Power BI semantic model/dataset ID\\n- question: Natural language question (e.g., \"What is the total NSR per product?\")\\n- auth_method: \"device_code\" or \"service_principal\" (default: \"device_code\")\\n\\nFor Service Principal auth, also provide:\\n- client_id: Azure AD application client ID\\n- tenant_id: Azure AD tenant ID\\n- client_secret: Service principal secret\\n\\nFor DAX Generation:\\n- databricks_host: Databricks workspace URL (e.g., \"https://example.databricks.com\")\\n- databricks_token: Databricks personal access token for LLM API\\n- model_name: LLM model to use (default: \"databricks-meta-llama-3-1-405b-instruct\")\\n- temperature: LLM temperature (default: 0.1)\\n\\nOptional Parameters:\\n- sample_size: Number of rows to sample per table for type inference (default: 100)\\n- skip_metadata: Skip metadata extraction if metadata is provided directly (default: False)\\n- metadata: Pre-extracted metadata (JSON string) - use with skip_metadata=True\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Power BI Full Pipeline - Metadata Extraction, DAX Generation, and Execution\n",
    "\n",
    "This notebook provides an end-to-end Power BI integration in one place:\n",
    "1. Extract metadata from Power BI semantic model\n",
    "2. Generate DAX query from natural language question using LLM\n",
    "3. Execute the generated DAX query\n",
    "4. Return all results\n",
    "\n",
    "Required Parameters (via job_params):\n",
    "- workspace_id: Power BI workspace ID\n",
    "- semantic_model_id: Power BI semantic model/dataset ID\n",
    "- question: Natural language question (e.g., \"What is the total NSR per product?\")\n",
    "- auth_method: \"device_code\" or \"service_principal\" (default: \"device_code\")\n",
    "\n",
    "For Service Principal auth, also provide:\n",
    "- client_id: Azure AD application client ID\n",
    "- tenant_id: Azure AD tenant ID\n",
    "- client_secret: Service principal secret\n",
    "\n",
    "For DAX Generation:\n",
    "- databricks_host: Databricks workspace URL (e.g., \"https://example.databricks.com\")\n",
    "- databricks_token: Databricks personal access token for LLM API\n",
    "- model_name: LLM model to use (default: \"databricks-meta-llama-3-1-405b-instruct\")\n",
    "- temperature: LLM temperature (default: 0.1)\n",
    "\n",
    "Optional Parameters:\n",
    "- sample_size: Number of rows to sample per table for type inference (default: 100)\n",
    "- skip_metadata: Skip metadata extraction if metadata is provided directly (default: False)\n",
    "- metadata: Pre-extracted metadata (JSON string) - use with skip_metadata=True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd471c22-271e-4931-8943-fa5affabeb2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-identity requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd3198d7-81bb-433f-bfd2-f1f2a99629db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dc50d3b-0900-4707-bee5-4438ddc37bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from azure.identity import DeviceCodeCredential, ClientSecretCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa356b0e-65a1-4698-8a32-37912d170af3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration - Get Job Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# Default configuration\n",
    "DEFAULT_TENANT_ID = \"9f37a392-f0ae-4280-9796-f1864a10effc\"\n",
    "DEFAULT_CLIENT_ID = \"1950a258-227b-4e31-a9cf-717495945fc2\"\n",
    "DEFAULT_MODEL_NAME = \"databricks-meta-llama-3-1-405b-instruct\"\n",
    "DEFAULT_TEMPERATURE = 0.1\n",
    "\n",
    "try:\n",
    "    # Get job parameters\n",
    "    job_params = json.loads(dbutils.widgets.get(\"job_params\"))\n",
    "\n",
    "    # Extract required parameters\n",
    "    WORKSPACE_ID = job_params.get(\"workspace_id\")\n",
    "    SEMANTIC_MODEL_ID = job_params.get(\"semantic_model_id\")\n",
    "    QUESTION = job_params.get(\"question\")\n",
    "\n",
    "    # Authentication configuration\n",
    "    AUTH_METHOD = job_params.get(\"auth_method\", \"device_code\")\n",
    "    TENANT_ID = job_params.get(\"tenant_id\", DEFAULT_TENANT_ID)\n",
    "    CLIENT_ID = job_params.get(\"client_id\", DEFAULT_CLIENT_ID)\n",
    "    CLIENT_SECRET = job_params.get(\"client_secret\")\n",
    "\n",
    "    # Databricks API configuration for LLM\n",
    "    DATABRICKS_HOST = job_params.get(\"databricks_host\")\n",
    "    DATABRICKS_TOKEN = job_params.get(\"databricks_token\")\n",
    "    MODEL_NAME = job_params.get(\"model_name\", DEFAULT_MODEL_NAME)\n",
    "    TEMPERATURE = job_params.get(\"temperature\", DEFAULT_TEMPERATURE)\n",
    "\n",
    "    # Optional parameters\n",
    "    SAMPLE_SIZE = job_params.get(\"sample_size\", 100)\n",
    "    SKIP_METADATA = job_params.get(\"skip_metadata\", False)\n",
    "    METADATA_JSON = job_params.get(\"metadata\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Power BI Full Pipeline - Metadata → DAX Generation → Execution\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Workspace ID: {WORKSPACE_ID}\")\n",
    "    print(f\"Semantic Model ID: {SEMANTIC_MODEL_ID}\")\n",
    "    print(f\"Question: {QUESTION}\")\n",
    "    print(f\"Authentication Method: {AUTH_METHOD}\")\n",
    "    print(f\"LLM Model: {MODEL_NAME}\")\n",
    "    print(f\"Temperature: {TEMPERATURE}\")\n",
    "    print(f\"Skip Metadata Extraction: {SKIP_METADATA}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error getting parameters: {str(e)}\")\n",
    "    print(\"\\nRequired parameters in job_params:\")\n",
    "    print(\"- workspace_id: Power BI workspace ID\")\n",
    "    print(\"- semantic_model_id: Power BI dataset/semantic model ID\")\n",
    "    print(\"- question: Natural language question\")\n",
    "    print(\"- databricks_host: Databricks workspace URL\")\n",
    "    print(\"- databricks_token: Databricks personal access token\")\n",
    "    print(\"\\nOptional parameters:\")\n",
    "    print(\"- auth_method: 'device_code' or 'service_principal' (default: 'device_code')\")\n",
    "    print(\"- model_name: LLM model (default: 'databricks-meta-llama-3-1-405b-instruct')\")\n",
    "    print(\"- temperature: LLM temperature (default: 0.1)\")\n",
    "    print(\"- sample_size: Rows to sample (default: 100)\")\n",
    "    print(\"- skip_metadata: Skip metadata extraction (default: False)\")\n",
    "    print(\"- metadata: Pre-extracted metadata JSON (use with skip_metadata=True)\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510775f1-4d0c-4e98-993d-36cf78a02fb8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Authentication Functions"
    }
   },
   "outputs": [],
   "source": [
    "def generate_token_device_code(tenant_id: str, client_id: str) -> str:\n",
    "    \"\"\"Generate token using device code flow (DCF).\"\"\"\n",
    "    try:\n",
    "        credential = DeviceCodeCredential(\n",
    "            client_id=client_id,\n",
    "            tenant_id=tenant_id,\n",
    "        )\n",
    "\n",
    "        print(\"\\n\uD83D\uDD04 Initiating Device Code Flow authentication...\")\n",
    "        print(\"⚠️  Follow the instructions above to authenticate\")\n",
    "\n",
    "        token = credential.get_token(\"https://analysis.windows.net/powerbi/api/.default\")\n",
    "\n",
    "        print(\"✅ Token generated successfully\")\n",
    "        return token.token\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Token generation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_token_service_principal(tenant_id: str, client_id: str, client_secret: str) -> str:\n",
    "    \"\"\"Generate token using Service Principal.\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\uD83D\uDD04 Authenticating with Service Principal...\")\n",
    "\n",
    "        credential = ClientSecretCredential(\n",
    "            tenant_id=tenant_id,\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret\n",
    "        )\n",
    "\n",
    "        token = credential.get_token(\"https://analysis.windows.net/powerbi/api/.default\")\n",
    "\n",
    "        print(\"✅ Token generated successfully\")\n",
    "        return token.token\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Service Principal authentication failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4074990-9c83-4326-b163-df9786f92e64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Access Token"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if AUTH_METHOD == \"service_principal\":\n",
    "        if not CLIENT_SECRET:\n",
    "            raise ValueError(\"client_secret is required for service_principal authentication\")\n",
    "\n",
    "        access_token = generate_token_service_principal(\n",
    "            tenant_id=TENANT_ID,\n",
    "            client_id=CLIENT_ID,\n",
    "            client_secret=CLIENT_SECRET\n",
    "        )\n",
    "    else:  # device_code (default)\n",
    "        access_token = generate_token_device_code(\n",
    "            tenant_id=TENANT_ID,\n",
    "            client_id=CLIENT_ID\n",
    "        )\n",
    "\n",
    "    print(f\"\\n✅ Authentication successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Authentication failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500967a8-75c4-4de9-8b8c-97f399276f8f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Power BI API Functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_info(token: str, dataset_id: str) -> dict:\n",
    "    \"\"\"Get basic dataset information.\"\"\"\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/datasets/{dataset_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    print(f\"\uD83D\uDD04 Fetching dataset information...\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Dataset information retrieved\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"❌ Failed to get dataset info: {response.text}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def execute_dax_for_metadata(token: str, dataset_id: str, dax_query: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute a DAX query to retrieve metadata.\"\"\"\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/datasets/{dataset_id}/executeQueries\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"queries\": [{\"query\": dax_query}],\n",
    "        \"serializerSettings\": {\"includeNulls\": True}\n",
    "    }\n",
    "\n",
    "    print(f\"   Executing query: {dax_query[:50]}...\")\n",
    "    response = requests.post(url, headers=headers, json=body, timeout=60)\n",
    "    print(f\"   Response status: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "        if results and results[0].get(\"tables\"):\n",
    "            rows = results[0][\"tables\"][0].get(\"rows\", [])\n",
    "            if rows:\n",
    "                return pd.DataFrame(rows)\n",
    "    else:\n",
    "        print(f\"   ❌ Query failed: {response.text}\")\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def extract_table_metadata_from_data(token: str, dataset_id: str, table_names: List[str], sample_size: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract column metadata by querying actual data from each table.\"\"\"\n",
    "    print(f\"\\n\uD83D\uDD04 Extracting metadata for {len(table_names)} tables...\")\n",
    "    print(f\"   Using sample size: {sample_size} rows per table\\n\")\n",
    "\n",
    "    tables_metadata = []\n",
    "\n",
    "    for table_name in table_names:\n",
    "        print(f\"   \uD83D\uDCCA Processing table: {table_name}\")\n",
    "\n",
    "        # Query sample data to get column names and types\n",
    "        query = f\"EVALUATE TOPN({sample_size}, '{table_name}')\"\n",
    "        df = execute_dax_for_metadata(token, dataset_id, query)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"   ⚠️  Could not query table: {table_name} (may be empty or not exist)\")\n",
    "            continue\n",
    "\n",
    "        # Extract column information from DataFrame\n",
    "        columns = []\n",
    "        for col_name in df.columns:\n",
    "            # Remove the table name prefix and square brackets from column names\n",
    "            clean_name = col_name.strip(table_name).strip('[').strip(']')\n",
    "\n",
    "            # Infer data type from pandas dtype based on actual data\n",
    "            dtype = str(df[col_name].dtype)\n",
    "\n",
    "            if 'object' in dtype or 'string' in dtype:\n",
    "                data_type = 'string'\n",
    "            elif 'int' in dtype:\n",
    "                data_type = 'int'\n",
    "            elif 'float' in dtype or 'decimal' in dtype:\n",
    "                data_type = 'decimal'\n",
    "            elif 'datetime' in dtype:\n",
    "                data_type = 'datetime'\n",
    "            elif 'bool' in dtype:\n",
    "                data_type = 'boolean'\n",
    "            else:\n",
    "                data_type = 'string'\n",
    "\n",
    "            columns.append({\n",
    "                \"name\": clean_name,\n",
    "                \"data_type\": data_type\n",
    "            })\n",
    "\n",
    "        tables_metadata.append({\n",
    "            \"name\": table_name,\n",
    "            \"columns\": columns\n",
    "        })\n",
    "\n",
    "        print(f\"   ✅ Found {len(columns)} columns in '{table_name}'\")\n",
    "\n",
    "    print(f\"\\n✅ Successfully processed {len(tables_metadata)}/{len(table_names)} tables\")\n",
    "\n",
    "    return tables_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d165432-620a-495b-ae1a-9424e4b2ebe2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "STEP 1: Extract Metadata (if not skipped)"
    }
   },
   "outputs": [],
   "source": [
    "if SKIP_METADATA and METADATA_JSON:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: Using Pre-Extracted Metadata\")\n",
    "    print(\"=\" * 80)\n",
    "    metadata = json.loads(METADATA_JSON)\n",
    "    tables_metadata = metadata.get(\"tables\", [])\n",
    "    print(f\"✅ Loaded metadata with {len(tables_metadata)} tables\")\n",
    "\n",
    "    # Create compact metadata\n",
    "    compact_metadata = {\n",
    "        \"tables\": [\n",
    "            {\n",
    "                \"name\": table[\"name\"],\n",
    "                \"columns\": [\n",
    "                    {\"name\": col[\"name\"], \"data_type\": col[\"data_type\"]}\n",
    "                    for col in table[\"columns\"]\n",
    "                ]\n",
    "            }\n",
    "            for table in tables_metadata\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    dataset_info = {\"name\": \"pre-loaded\"}\n",
    "    total_columns = sum(len(table[\"columns\"]) for table in tables_metadata)\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: Extracting Metadata from Power BI\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        # Get dataset info\n",
    "        dataset_info = get_dataset_info(access_token, SEMANTIC_MODEL_ID)\n",
    "\n",
    "        if dataset_info:\n",
    "            print(f\"Dataset Name: {dataset_info.get('name', 'N/A')}\")\n",
    "\n",
    "        # Discover tables\n",
    "        print(\"\\n\uD83D\uDD04 Discovering tables in dataset...\")\n",
    "        tables_df = execute_dax_for_metadata(access_token, SEMANTIC_MODEL_ID, \"EVALUATE INFO.VIEW.TABLES()\")\n",
    "\n",
    "        if tables_df.empty:\n",
    "            raise ValueError(\"No tables found in dataset\")\n",
    "\n",
    "        # Extract unique table names\n",
    "        table_names = list(set(tables_df['[Name]']))\n",
    "        print(f\"Found {len(table_names)} tables: {', '.join(table_names[:5])}\")\n",
    "\n",
    "        # Extract metadata\n",
    "        tables_metadata = extract_table_metadata_from_data(\n",
    "            token=access_token,\n",
    "            dataset_id=SEMANTIC_MODEL_ID,\n",
    "            table_names=table_names,\n",
    "            sample_size=SAMPLE_SIZE\n",
    "        )\n",
    "\n",
    "        # Build metadata structure\n",
    "        metadata = {\"tables\": tables_metadata}\n",
    "\n",
    "        # Create compact metadata\n",
    "        compact_metadata = {\n",
    "            \"tables\": [\n",
    "                {\n",
    "                    \"name\": table[\"name\"],\n",
    "                    \"columns\": [\n",
    "                        {\"name\": col[\"name\"], \"data_type\": col[\"data_type\"]}\n",
    "                        for col in table[\"columns\"]\n",
    "                    ]\n",
    "                }\n",
    "                for table in tables_metadata\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        total_columns = sum(len(table[\"columns\"]) for table in tables_metadata)\n",
    "\n",
    "        print(f\"\\n✅ STEP 1 Complete: Extracted {len(tables_metadata)} tables, {total_columns} columns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in metadata extraction: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a75da11e-cd60-4bf2-b9a5-6add3541cdbd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Metadata Formatting for LLM"
    }
   },
   "outputs": [],
   "source": [
    "def format_metadata_for_llm(metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format metadata as a readable string for LLM context.\"\"\"\n",
    "    tables = metadata.get('tables', [])\n",
    "    if not tables:\n",
    "        return \"No metadata available\"\n",
    "\n",
    "    output = \"Power BI Dataset Structure:\\n\\n\"\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table.get('name', 'Unknown')\n",
    "        columns = table.get('columns', [])\n",
    "\n",
    "        output += f\"Table: {table_name}\\n\"\n",
    "\n",
    "        if columns:\n",
    "            output += \"Columns:\\n\"\n",
    "            for col in columns:\n",
    "                col_name = col.get('name', 'Unknown')\n",
    "                col_type = col.get('data_type', 'Unknown')\n",
    "                output += f\"  - {col_name} ({col_type})\\n\"\n",
    "\n",
    "        output += \"\\n\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1553277d-abd8-4685-a8b3-c58db2e8211e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DAX Query Cleaning Utility"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dax_query(dax_query: str) -> str:\n",
    "    \"\"\"Remove HTML/XML tags and other artifacts from DAX queries.\"\"\"\n",
    "    # Remove HTML/XML tags like <oii>, </oii>, etc.\n",
    "    cleaned = re.sub(r\"<[^>]+>\", \"\", dax_query)\n",
    "    # Collapse extra whitespace\n",
    "    cleaned = \" \".join(cleaned.split())\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c2d1a10-fec5-45ef-97b5-726b2691d605",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "STEP 2: Generate DAX Query from Question"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Generating DAX Query from Natural Language Question\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {QUESTION}\")\n",
    "\n",
    "try:\n",
    "    # Format metadata for LLM\n",
    "    metadata_str = format_metadata_for_llm(metadata)\n",
    "\n",
    "    # Build prompt for DAX generation\n",
    "    prompt = f\"\"\"You are a Power BI DAX expert. Generate a DAX query to answer the following question.\n",
    "\n",
    "Available dataset structure:\n",
    "{metadata_str}\n",
    "\n",
    "User question: {QUESTION}\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Generate only the DAX query without any explanation or markdown\n",
    "2. Do NOT use any HTML or XML tags in the query\n",
    "3. Do NOT use angle brackets < or > except for DAX operators\n",
    "4. Use only valid DAX syntax\n",
    "5. Reference only columns and measures that exist in the schema\n",
    "6. The query should be executable as-is\n",
    "7. Use proper DAX functions like EVALUATE, SUMMARIZE, FILTER, CALCULATE, etc.\n",
    "8. Start the query with EVALUATE\n",
    "\n",
    "Example format:\n",
    "EVALUATE SUMMARIZE(Sales, Product[Category], \"Total Revenue\", SUM(Sales[Amount]))\n",
    "\n",
    "Now generate the DAX query for the user's question:\"\"\"\n",
    "\n",
    "    print(f\"\\n\uD83D\uDD04 Calling LLM: {MODEL_NAME}\")\n",
    "    print(f\"   Temperature: {TEMPERATURE}\")\n",
    "\n",
    "    # Call Databricks LLM API\n",
    "    llm_url = f\"{DATABRICKS_HOST}/serving-endpoints/{MODEL_NAME}/invocations\"\n",
    "    llm_headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    llm_body = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    print(f\"   Endpoint: {llm_url}\")\n",
    "    llm_response = requests.post(llm_url, headers=llm_headers, json=llm_body, timeout=120)\n",
    "\n",
    "    if llm_response.status_code != 200:\n",
    "        raise Exception(f\"LLM API call failed: {llm_response.text}\")\n",
    "\n",
    "    llm_result = llm_response.json()\n",
    "\n",
    "    # Extract content from response\n",
    "    if \"choices\" in llm_result and len(llm_result[\"choices\"]) > 0:\n",
    "        raw_dax = llm_result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected LLM response format: {llm_result}\")\n",
    "\n",
    "    print(\"✅ LLM response received\")\n",
    "\n",
    "    # Clean the response\n",
    "    cleaned_dax = clean_dax_query(raw_dax)\n",
    "\n",
    "    # Remove markdown code blocks if present\n",
    "    if \"```\" in cleaned_dax:\n",
    "        parts = cleaned_dax.split(\"```\")\n",
    "        for part in parts:\n",
    "            if \"EVALUATE\" in part.upper():\n",
    "                cleaned_dax = part.strip()\n",
    "                # Remove language identifier if present\n",
    "                if cleaned_dax.startswith(\"dax\\n\") or cleaned_dax.startswith(\"DAX\\n\"):\n",
    "                    cleaned_dax = cleaned_dax[4:].strip()\n",
    "                break\n",
    "\n",
    "    # Ensure query starts with EVALUATE\n",
    "    if not cleaned_dax.strip().upper().startswith(\"EVALUATE\"):\n",
    "        lines = cleaned_dax.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"EVALUATE\" in line.upper():\n",
    "                cleaned_dax = \"\\n\".join(lines[i:])\n",
    "                break\n",
    "\n",
    "    DAX_QUERY = cleaned_dax.strip()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Generated DAX Query:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(DAX_QUERY)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\n✅ STEP 2 Complete: DAX query generated successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in DAX generation: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b2b932-dc5a-497c-a135-a971cbf233a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "STEP 3: Execute DAX Query"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Executing DAX Query\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def execute_dax_query(token: str, dataset_id: str, dax_query: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute a DAX query against the Power BI dataset using REST API.\"\"\"\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/datasets/{dataset_id}/executeQueries\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"queries\": [\n",
    "            {\n",
    "                \"query\": dax_query\n",
    "            }\n",
    "        ],\n",
    "        \"serializerSettings\": {\n",
    "            \"includeNulls\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"\\n\uD83D\uDD04 Executing DAX query...\")\n",
    "    print(f\"   Endpoint: {url}\")\n",
    "    print(f\"   Query preview: {dax_query[:100]}...\")\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=body, timeout=60)\n",
    "    print(f\"   Response status: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "\n",
    "        if results and results[0].get(\"tables\"):\n",
    "            rows = results[0][\"tables\"][0].get(\"rows\", [])\n",
    "\n",
    "            if rows:\n",
    "                df = pd.DataFrame(rows)\n",
    "                print(f\"✅ Query successful: {len(df)} rows returned\")\n",
    "                print(f\"   Columns: {list(df.columns)}\")\n",
    "                return df\n",
    "            else:\n",
    "                print(\"⚠️  Query returned no rows\")\n",
    "                return pd.DataFrame()\n",
    "        else:\n",
    "            print(\"⚠️  No tables in response\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"❌ Query failed: {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    df_result = execute_dax_query(access_token, SEMANTIC_MODEL_ID, DAX_QUERY)\n",
    "\n",
    "    if not df_result.empty:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Query Results\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total rows: {len(df_result)}\")\n",
    "        print(f\"Columns: {list(df_result.columns)}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Display first few rows\n",
    "        print(\"\\nSample Results (First 10 rows):\")\n",
    "        print(\"-\" * 80)\n",
    "        display(df_result.head(10))\n",
    "\n",
    "        if len(df_result) > 10:\n",
    "            print(f\"\\n... and {len(df_result) - 10} more rows\")\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        print(\"\\n\uD83D\uDD04 Converting results to Spark DataFrame...\")\n",
    "        spark_df = spark.createDataFrame(df_result)\n",
    "        print(f\"✅ Spark DataFrame created successfully\")\n",
    "\n",
    "        print(f\"\\n✅ STEP 3 Complete: Query executed, {len(df_result)} rows returned\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️  Query returned empty DataFrame\")\n",
    "        spark_df = None\n",
    "        print(f\"\\n⚠️  STEP 3 Complete: No results returned\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error executing DAX query: {str(e)}\")\n",
    "    df_result = pd.DataFrame()\n",
    "    spark_df = None\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b65608b-334e-4475-9b47-82cf92507ee6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execution Summary"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FULL PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ Pipeline Completed Successfully\")\n",
    "print(f\"   Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Workspace ID: {WORKSPACE_ID}\")\n",
    "print(f\"   Semantic Model ID: {SEMANTIC_MODEL_ID}\")\n",
    "print(f\"   Dataset Name: {dataset_info.get('name', 'N/A')}\")\n",
    "print(\"\")\n",
    "print(f\"STEP 1 - Metadata Extraction:\")\n",
    "print(f\"   Tables: {len(tables_metadata)}\")\n",
    "print(f\"   Total Columns: {total_columns}\")\n",
    "print(\"\")\n",
    "print(f\"STEP 2 - DAX Generation:\")\n",
    "print(f\"   Question: {QUESTION}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Generated Query Length: {len(DAX_QUERY)} characters\")\n",
    "print(\"\")\n",
    "print(f\"STEP 3 - DAX Execution:\")\n",
    "print(f\"   Authentication: {AUTH_METHOD}\")\n",
    "print(f\"   Rows Returned: {len(df_result) if not df_result.empty else 0}\")\n",
    "print(f\"   Columns: {list(df_result.columns) if not df_result.empty else 'N/A'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d663111-5ea7-4b8a-afb2-3bc8e176b428",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Return Results as JSON"
    }
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame to JSON for output\n",
    "if not df_result.empty:\n",
    "    # Convert to list of dictionaries\n",
    "    result_data = df_result.to_dict(orient='records')\n",
    "else:\n",
    "    result_data = []\n",
    "\n",
    "# Build complete result summary\n",
    "result_summary = {\n",
    "    \"status\": \"success\",\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"pipeline_steps\": {\n",
    "        \"step_1_metadata\": {\n",
    "            \"tables_count\": len(tables_metadata),\n",
    "            \"columns_count\": total_columns,\n",
    "            \"metadata\": metadata,\n",
    "            \"compact_metadata\": compact_metadata\n",
    "        },\n",
    "        \"step_2_dax_generation\": {\n",
    "            \"question\": QUESTION,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"generated_dax\": DAX_QUERY\n",
    "        },\n",
    "        \"step_3_execution\": {\n",
    "            \"workspace_id\": WORKSPACE_ID,\n",
    "            \"semantic_model_id\": SEMANTIC_MODEL_ID,\n",
    "            \"auth_method\": AUTH_METHOD,\n",
    "            \"rows_returned\": len(df_result) if not df_result.empty else 0,\n",
    "            \"columns\": list(df_result.columns) if not df_result.empty else [],\n",
    "            \"result_data\": result_data[:1000]  # Limit to first 1000 rows for JSON output\n",
    "        }\n",
    "    },\n",
    "    \"dataset_name\": dataset_info.get('name', 'unknown')\n",
    "}\n",
    "\n",
    "# Exit with complete results\n",
    "dbutils.notebook.exit(json.dumps(result_summary))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "powerbi_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}